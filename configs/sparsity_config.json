{
  "static_pruning": {
    "sparsity": 0.8,
    "target_layers": ["weight"],
    "method": "magnitude",
    "chunk_size": 1000000
  },
  "cyclic_sparsity": {
    "min_sparsity": 0.8,
    "max_sparsity": 0.9,
    "cycle": 5,
    "apply_to": ["attention", "ffn"]
  },
  "dynamic_relu": {
    "enabled": true,
    "start_epoch": 0,
    "end_epoch": 75
  },
  "weight_sharing": {
    "enabled": true,
    "share_ratio": 0.5
  },
  "training": {
    "learning_rate": 5e-5,
    "batch_size": 2,
    "gradient_accumulation_steps": 4,
    "weight_decay": 0.01
  }
}
